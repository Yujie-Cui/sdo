/*
 * Copyright (c) 2013 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

machine(MachineType:L0Cache, "MESI Directory L0 Cache")
 : Sequencer * sequencer;
   CacheMemory * Icache;
   CacheMemory * Dcache;
   Prefetcher * prefetcher;
   Cycles request_latency := 1;
   Cycles response_latency := 1;
   bool send_evictions;
   bool enable_prefetch := "False";

   // From this node's L0 cache to the network
   MessageBuffer * bufferToL1, network="To";

   // To this node's L0 cache FROM the network
   MessageBuffer * bufferFromL1, network="From";

   // Jiyong, MLDOM: spec data channel from L1 to L0
   MessageBuffer * bufferFromL1_specData, network="From";

   // Message queue between this controller and the processor
   // for requests generated by the processor core
   MessageBuffer * mandatoryQueue;

   // Message queue owned by this controller
   // for prefetch requests
   MessageBuffer * optionalQueue;
{
  // STATES
  state_declaration(State, desc="Cache states", default="L0Cache_State_I") {
    // Base states

    // The cache entry has not been allocated.
    I, AccessPermission:Invalid;

    // The cache entry is in shared mode. The processor can read this entry
    // but it cannot write to it.
    S, AccessPermission:Read_Only;

    // The cache entry is in exclusive mode. The processor can read this
    // entry. It can write to this entry without informing the directory.
    // On writing, the entry moves to M state.
    E, AccessPermission:Read_Only;

    // The processor has read and write permissions on this entry.
    M, AccessPermission:Read_Write;

    // Transient States

    // The cache controller has requested an instruction.  It will be stored
    // in the shared state so that the processor can read it.
    Inst_IS, AccessPermission:Busy;

    // The cache controller has requested that this entry be fetched in
    // shared state so that the processor can read it.
    IS, AccessPermission:Busy;

    // The cache controller has requested that this entry be fetched in
    // modify state so that the processor can read/write it.
    IM, AccessPermission:Busy;

    // The cache controller had read permission over the entry. But now the
    // processor needs to write to it. So, the controller has requested for
    // write permission.
    SM, AccessPermission:Read_Only;

    // Transient States in which block is being prefetched
    PF_IS, AccessPermission:Busy,      desc="Issued GETS, have not seen response yet";
    PF_Inst_IS, AccessPermission:Busy, desc="Issued GetS, have not seen response yet";
    PF_IM, AccessPermission:Busy,      desc="Issued GETX, have not seen response yet";
  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // L0 events
    Load,            desc="Load request from the home processor";
    Ifetch,          desc="I-fetch request from the home processor";
    Store,           desc="Store request from the home processor";
    // Jiyong, MLDOM
    SpecLoad_L0,            desc="Specload request from home processor, all the way till L0";
    SpecLoad_L1,            desc="Specload request from home processor, all the way till L1";
    SpecLoad_L2,            desc="Specload request from home processor, all the way till L2";
    SpecLoad_Mem,           desc="Specload request from home processor, all the way till Memory";
    SpecLoad_Perfect,       desc="Specload request from home processor, perfectly hit the location";
    SpecLoad_PerfectUnsafe, desc="Specload request from home processor, perfectly hit the location(unsafe version)";

    Inv,           desc="Invalidate request from L2 bank";

    // internal generated request
    L0D_Replacement,  desc="L0D Replacement", format="!r";
    L0I_Replacement,  desc="L0I Replacement", format="!r";
    PF_L0D_Replacement, desc="Prefetch L0D Replacement", format="!pr";
    PF_L0I_Replacement, desc="Prefetch L0I Replacement", format="!pr";

    // other requests
    Fwd_GETX,   desc="GETX from other processor";
    Fwd_GETS,   desc="GETS from other processor";
    Fwd_GET_INSTR,   desc="GET_INSTR from other processor";

    Data,               desc="Data for processor";
    Data_Exclusive,     desc="Data for processor";
    // Jiyong, MLDOM
    Data_Spec_fromL1,   desc="Spec data for processor sent from L1";
    Data_Spec_fromL2,   desc="Spec data for processor sent from L2";
    Data_Spec_fromMem,  desc="Spec data for processor sent from Mem";

    Ack,        desc="Ack for processor";
    Ack_all,      desc="Last ack for processor";

    WB_Ack,        desc="Ack for replacement";

    PF_Load,    desc="load request from prefetcher";
    PF_Ifetch,  desc="instruction fetch request from prefetcher";
    PF_Store,   desc="exclusive load request from prefetcher";
  }

  enumeration(RequestType, desc="To communicate stats from transitions to record Stats") {
    L0DDataArrayRead,   desc="Read the L0 Dcache data array";
    L0DDataArrayWrite,  desc="Write the L0 Dcache data array";
    L0DTagArrayRead,    desc="Read the L0 Dcache tag array";
    L0DTagArrayWrite,   desc="Write the L0 Dcache tag array";
    L0IDataArrayRead,   desc="Read the L0 Icache data array";
    L0IDataArrayWrite,  desc="Write the L0 Icache data array";
    L0ITagArrayRead,    desc="Read the L0 Icache tag array";
    L0ITagArrayWrite,   desc="Write the L0 Icache tag array";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,            desc="cache state";
    DataBlock DataBlk,           desc="data for the block";
    bool Dirty, default="false", desc="data is dirty";
    bool isPrefetch,             desc="set if the block was prefetched";
    bool hitAtL0,  default="false", desc="if the spec load was a L0 hit";  // Jiyong MLDOM
    bool hitAtL1,  default="false", desc="if the spec load was a L1 hit";  // Jiyong MLDOM
    bool hitAtL2,  default="false", desc="if the spec load was a L2 hit";  // Jiyong MLDOM
    bool hitAtMem, default="false", desc="if the spec load was a Mem hit"; // Jiyong MLDOM
  }

  // TBE fields
  structure(TBE, desc="...") {
    Addr addr,                    desc="Physical address for this TBE";
    State TBEState,               desc="Transient state";
    DataBlock DataBlk,            desc="Buffer for the data block";
    bool Dirty, default="false",  desc="data is dirty";
    bool isPrefetch,              desc="set if this was caused by a prefetch";
    int pendingAcks, default="0", desc="number of pending acks";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Addr);
    void allocate(Addr);
    void deallocate(Addr);
    bool isPresent(Addr);
  }

  TBETable TBEs, template="<L0Cache_TBE>", constructor="m_number_of_TBEs";

  Tick clockEdge();
  Cycles ticksToCycles(Tick t);
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Addr a);
  void wakeUpAllBuffers(Addr a);
  void profileMsgDelay(int virtualNetworkType, Cycles c);

  // inclusive cache returns L0 entries only
  Entry getCacheEntry(Addr addr), return_by_pointer="yes" {
    Entry Dcache_entry := static_cast(Entry, "pointer", Dcache[addr]);
    if(is_valid(Dcache_entry)) {
      return Dcache_entry;
    }

    Entry Icache_entry := static_cast(Entry, "pointer", Icache[addr]);
    return Icache_entry;
  }

  Entry getDCacheEntry(Addr addr), return_by_pointer="yes" {
    Entry Dcache_entry := static_cast(Entry, "pointer", Dcache[addr]);
    return Dcache_entry;
  }

  Entry getICacheEntry(Addr addr), return_by_pointer="yes" {
    Entry Icache_entry := static_cast(Entry, "pointer", Icache[addr]);
    return Icache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Addr addr) {
    assert((Dcache.isTagPresent(addr) && Icache.isTagPresent(addr)) == false);

    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:I;
  }

  void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {
    assert((Dcache.isTagPresent(addr) && Icache.isTagPresent(addr)) == false);

    // MUST CHANGE
    if(is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Addr addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s\n", L0Cache_State_to_permission(tbe.TBEState));
      return L0Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L0Cache_State_to_permission(cache_entry.CacheState));
      return L0Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    return AccessPermission:NotPresent;
  }

  void functionalRead(Addr addr, Packet *pkt) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      testAndRead(addr, tbe.DataBlk, pkt);
    } else {
      testAndRead(addr, getCacheEntry(addr).DataBlk, pkt);
    }
  }

  int functionalWrite(Addr addr, Packet *pkt) {
    int num_functional_writes := 0;

    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      num_functional_writes := num_functional_writes +
        testAndWrite(addr, tbe.DataBlk, pkt);
      return num_functional_writes;
    }

    num_functional_writes := num_functional_writes +
        testAndWrite(addr, getCacheEntry(addr).DataBlk, pkt);
    return num_functional_writes;
  }

  void setAccessPermission(Entry cache_entry, Addr addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L0Cache_State_to_permission(state));
    }
  }

  // Jiyong, MLDOM: add extra events
  Event mandatory_request_type_to_event(RubyRequestType type) {
    if (type == RubyRequestType:LD) {
      return Event:Load;
    } else if (type == RubyRequestType:IFETCH) {
      return Event:Ifetch;
    } else if ((type == RubyRequestType:ST) || (type == RubyRequestType:ATOMIC)) {
      return Event:Store;
    } else if (type == RubyRequestType:SPEC_LD_L0) {
      return Event:SpecLoad_L0;
    } else if (type == RubyRequestType:SPEC_LD_L1) {
      return Event:SpecLoad_L1;
    } else if (type == RubyRequestType:SPEC_LD_L2) {
      return Event:SpecLoad_L2;
    } else if (type == RubyRequestType:SPEC_LD_Mem) {
      return Event:SpecLoad_Mem;
    } else if (type == RubyRequestType:SPEC_LD_Perfect) {
      return Event:SpecLoad_Perfect;
    } else if (type == RubyRequestType:SPEC_LD_PerfectUnsafe) {
      return Event:SpecLoad_PerfectUnsafe;
    } else {
      error("Invalid RubyRequestType");
    }
  }

  Event prefetch_request_type_to_event(RubyRequestType type) {
    if (type == RubyRequestType:LD) {
        return Event:PF_Load;
    } else if (type == RubyRequestType:IFETCH) {
        return Event:PF_Ifetch;
    } else if ((type == RubyRequestType:ST) ||
               (type == RubyRequestType:ATOMIC)) {
        return Event:PF_Store;
    } else {
        error("Invalid RubyRequestType");
    }
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  void recordRequestType(RequestType request_type, Addr addr) {
    if (request_type == RequestType:L0DDataArrayRead) {
      Dcache.recordRequestType(CacheRequestType:DataArrayRead, addr);
    } else if (request_type == RequestType:L0DDataArrayWrite) {
      Dcache.recordRequestType(CacheRequestType:DataArrayWrite, addr);
    } else if (request_type == RequestType:L0DTagArrayRead) {
      Dcache.recordRequestType(CacheRequestType:TagArrayRead, addr);
    } else if (request_type == RequestType:L0DTagArrayWrite) {
      Dcache.recordRequestType(CacheRequestType:TagArrayWrite, addr);
    } else if (request_type == RequestType:L0IDataArrayRead) {
      Icache.recordRequestType(CacheRequestType:DataArrayRead, addr);
    } else if (request_type == RequestType:L0IDataArrayWrite) {
      Icache.recordRequestType(CacheRequestType:DataArrayWrite, addr);
    } else if (request_type == RequestType:L0ITagArrayRead) {
      Icache.recordRequestType(CacheRequestType:TagArrayRead, addr);
    } else if (request_type == RequestType:L0ITagArrayWrite) {
      Icache.recordRequestType(CacheRequestType:TagArrayWrite, addr);
    }
  }

  bool checkResourceAvailable(RequestType request_type, Addr addr) {
    if (request_type == RequestType:L0DDataArrayRead) {
      return Dcache.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (request_type == RequestType:L0DDataArrayWrite) {
      return Dcache.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (request_type == RequestType:L0DTagArrayRead) {
      return Dcache.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else if (request_type == RequestType:L0DTagArrayWrite) {
      return Dcache.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else if (request_type == RequestType:L0IDataArrayRead) {
      return Icache.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (request_type == RequestType:L0IDataArrayWrite) {
      return Icache.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (request_type == RequestType:L0ITagArrayRead) {
      return Icache.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else if (request_type == RequestType:L0ITagArrayWrite) {
      return Icache.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else {
      return true;
    }
  }

  out_port(requestNetwork_out, CoherenceMsg, bufferToL1);
  out_port(optionalQueue_out, RubyRequest, optionalQueue);

  // Prefetch queue between the controller and the prefetcher
  // As per Spracklen et al. (HPCA 2005), the prefetch queue should be
  // implemented as a LIFO structure.  The structure would allow for fast
  // searches of all entries in the queue, not just the head msg. All
  // msgs in the structure can be invalidated if a demand miss matches.
  in_port(optionalQueue_in, RubyRequest, optionalQueue, desc="...", rank = 3) {
      if (optionalQueue_in.isReady(clockEdge())) {
          peek(optionalQueue_in, RubyRequest) {
              if (in_msg.Type == RubyRequestType:IFETCH) {
                  // Instruction Prefetch
                  Entry Icache_entry := getICacheEntry(in_msg.LineAddress);
                  if (is_valid(Icache_entry)) {
                      // The block to be prefetched is already present in the
                      // cache. We should drop this request.
                      trigger(prefetch_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                              Icache_entry, TBEs[in_msg.LineAddress]);
                  } else {

                      // Check to see if it is in the OTHER L0
                      Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
                      if (is_valid(Dcache_entry)) {
                          // The block is in the wrong L0 cache. We should drop this request.
                          trigger(prefetch_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                              Dcache_entry, TBEs[in_msg.LineAddress]);
                      }

                      if (Icache.cacheAvail(in_msg.LineAddress)) {
                          // L0 does't have the line, but we have space for it
                          // in the L0 so let's see if the L2 has it
                          trigger(prefetch_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                              Icache_entry, TBEs[in_msg.LineAddress]);
                      } else {
                          // No room in the L0, so we need to make room in the L0
                          Addr victim := Icache.cacheProbe(in_msg.LineAddress);
                          trigger(Event:PF_L0I_Replacement, victim, getICacheEntry(victim), TBEs[victim]);
                      }
                  }
              } else {
                  // Data prefetch
                  Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
                  if (is_valid(Dcache_entry)) {
                      // The block to be prefetched is already present in the 
                      // cache. We should drop this request.
                      trigger(prefetch_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                              Dcache_entry, TBEs[in_msg.LineAddress]);
                  } else {

                      // Check to see if it is in the OTHER L0
                      Entry Icache_entry := getICacheEntry(in_msg.LineAddress);
                      if (is_valid(Icache_entry)) {
                          // The block is in the wrong L0. Just drop the prefetch request.
                          trigger(prefetch_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                                  Icache_entry, TBEs[in_msg.LineAddress]);
                      }

                      if (Dcache.cacheAvail(in_msg.LineAddress)) {
                          // L0 does't have the line, but we have space for it in
                          // the L0 let's see if the L2 has it
                          trigger(prefetch_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                                  Dcache_entry, TBEs[in_msg.LineAddress]);
                      } else {
                          // No room in the L1, so we need to make room in the L1
                          Addr victim := Dcache.cacheProbe(in_msg.LineAddress);
                          trigger(Event:PF_L0D_Replacement, victim, getDCacheEntry(victim), TBEs[victim]);
                      }
                  }
              }
          }
      }
  }

  // Messages for this L0 cache from the L1 cache
  in_port(messgeBuffer_in, CoherenceMsg, bufferFromL1, rank = 1) {
    if (messgeBuffer_in.isReady(clockEdge())) {
      peek(messgeBuffer_in, CoherenceMsg, block_on="addr") {
        assert(in_msg.Dest == machineID);

        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        if(in_msg.Class == CoherenceClass:DATA_EXCLUSIVE) {
            trigger(Event:Data_Exclusive, in_msg.addr, cache_entry, tbe);
        } else if(in_msg.Class == CoherenceClass:DATA) {
            trigger(Event:Data, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:ACK) {
            trigger(Event:Ack, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:WB_ACK) {
            trigger(Event:WB_Ack, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:INV) {
          trigger(Event:Inv, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:GETX ||
                   in_msg.Class == CoherenceClass:UPGRADE) {
          // upgrade transforms to GETX due to race
          trigger(Event:Fwd_GETX, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:GETS) {
          trigger(Event:Fwd_GETS, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Class == CoherenceClass:GET_INSTR) {
          trigger(Event:Fwd_GET_INSTR, in_msg.addr, cache_entry, tbe);
        } else {
          error("Invalid forwarded request type");
        }
      }
    }
  }

  // Jiyong, MLDOM: spec data will arrive at this port
  in_port(messgeBuffer_specData_in, CoherenceMsg, bufferFromL1_specData, rank = 2) {
    if (messgeBuffer_specData_in.isReady(clockEdge())) {
      peek(messgeBuffer_specData_in, CoherenceMsg, block_on="addr") {
        assert(in_msg.Dest == machineID);

        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        if(in_msg.Class == CoherenceClass:DATA_SPEC_FROM_L1) {
            trigger(Event:Data_Spec_fromL1, in_msg.addr, cache_entry, tbe);
        } else if(in_msg.Class == CoherenceClass:DATA_SPEC_FROM_L2) {
            trigger(Event:Data_Spec_fromL2, in_msg.addr, cache_entry, tbe);
        } else if(in_msg.Class == CoherenceClass:DATA_SPEC_FROM_MEM) {
            trigger(Event:Data_Spec_fromMem, in_msg.addr, cache_entry, tbe);
        } else {
          error("Invalid forwarded request type");
        }
      }
    }
  }

  // Mandatory Queue betweens Node's CPU and it's L0 caches
  in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...", rank = 0) {
    if (mandatoryQueue_in.isReady(clockEdge())) {
      peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {

        // Check for data access to blocks in I-cache and ifetchs to blocks in D-cache
        // Jiyong: for spec_lds we simple convert them into accesses
        if (in_msg.Type == RubyRequestType:SPEC_LD_L0) {
          Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
          trigger(Event:SpecLoad_L0, in_msg.LineAddress, Dcache_entry, TBEs[in_msg.LineAddress]);
        }
        else if (in_msg.Type == RubyRequestType:SPEC_LD_L1) {
          Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
          trigger(Event:SpecLoad_L1, in_msg.LineAddress, Dcache_entry, TBEs[in_msg.LineAddress]);
        }
        else if (in_msg.Type == RubyRequestType:SPEC_LD_L2) {
          Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
          trigger(Event:SpecLoad_L2, in_msg.LineAddress, Dcache_entry, TBEs[in_msg.LineAddress]);
        }
        else if (in_msg.Type == RubyRequestType:SPEC_LD_Mem) {
          Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
          trigger(Event:SpecLoad_Mem, in_msg.LineAddress, Dcache_entry, TBEs[in_msg.LineAddress]);
        }
        else if (in_msg.Type == RubyRequestType:SPEC_LD_Perfect) {
          Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
          trigger(Event:SpecLoad_Perfect, in_msg.LineAddress, Dcache_entry, TBEs[in_msg.LineAddress]);
        }
        else if (in_msg.Type == RubyRequestType:SPEC_LD_PerfectUnsafe) {
          Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
          trigger(Event:SpecLoad_PerfectUnsafe, in_msg.LineAddress, Dcache_entry, TBEs[in_msg.LineAddress]);
        }
        else if (in_msg.Type == RubyRequestType:IFETCH) {
          // ** INSTRUCTION ACCESS ***

          Entry Icache_entry := getICacheEntry(in_msg.LineAddress);
          if (is_valid(Icache_entry)) {
            // The tag matches for the L0, so the L0 asks the L2 for it.
            trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                    Icache_entry, TBEs[in_msg.LineAddress]);
          } else {

            // Check to see if it is in the OTHER L0
            Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
            if (is_valid(Dcache_entry)) {
              // The block is in the wrong L0, put the request on the queue to the shared L2
              trigger(Event:L0D_Replacement, in_msg.LineAddress,
                      Dcache_entry, TBEs[in_msg.LineAddress]);
            }

            if (Icache.cacheAvail(in_msg.LineAddress)) {
              // L0 does't have the line, but we have space for it
              // in the L0 so let's see if the L2 has it
              trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                      Icache_entry, TBEs[in_msg.LineAddress]);
            } else {
              // No room in the L0, so we need to make room in the L0
              trigger(Event:L0I_Replacement, Icache.cacheProbe(in_msg.LineAddress),
                      getICacheEntry(Icache.cacheProbe(in_msg.LineAddress)),
                      TBEs[Icache.cacheProbe(in_msg.LineAddress)]);
            }
          }
        } else {

          // *** DATA ACCESS ***
          Entry Dcache_entry := getDCacheEntry(in_msg.LineAddress);
          if (is_valid(Dcache_entry)) {
            // The tag matches for the L0, so the L0 ask the L1 for it
            trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                    Dcache_entry, TBEs[in_msg.LineAddress]);
          } else {

            // Check to see if it is in the OTHER L0
            Entry Icache_entry := getICacheEntry(in_msg.LineAddress);
            if (is_valid(Icache_entry)) {
              // The block is in the wrong L0, put the request on the queue to the private L1
              trigger(Event:L0I_Replacement, in_msg.LineAddress,
                      Icache_entry, TBEs[in_msg.LineAddress]);
            }

            if (Dcache.cacheAvail(in_msg.LineAddress)) {
              // L1 does't have the line, but we have space for it
              // in the L0 let's see if the L1 has it
              trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                      Dcache_entry, TBEs[in_msg.LineAddress]);
            } else {
              // No room in the L1, so we need to make room in the L0
              trigger(Event:L0D_Replacement, Dcache.cacheProbe(in_msg.LineAddress),
                      getDCacheEntry(Dcache.cacheProbe(in_msg.LineAddress)),
                      TBEs[Dcache.cacheProbe(in_msg.LineAddress)]);
            }
          }
        }
      }
    }
  }

  // add prefetch requests to prefetch queue (optionalQueue)
  void enqueuePrefetch(Addr address, RubyRequestType type) {
      enqueue(optionalQueue_out, RubyRequest, 1) {
          out_msg.LineAddress := address;
          out_msg.Type := type;
          out_msg.AccessMode := RubyAccessMode:Supervisor;
      }
  }


  // ACTIONS
  action(a_issueGETS, "a", desc="Issue GETS") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETS;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.Prefetch := in_msg.Prefetch;
      }
    }
  }

  // Jiyong, MLDOM: SpecLoad_L1 hit/miss L0 must send request to lower levels
  action(ij_issueGETSPEC_L1_pass_L0, '1p0', desc="Issue SpecLoad L1 to lower levels given it hits/misses L0") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETSPEC_L1;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:SPECLD_L1_Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.idx := in_msg.idx;
        out_msg.Prefetch := in_msg.Prefetch;
      } // enqueue
    } // peek
  }
  // Jiyong, MLDOM: SpecLoad_L2 hit/miss L0 must send request to lower levels
  action(ij_issueGETSPEC_L2_pass_L0, '2p0', desc="Issue SpecLoad L2 to lower levels given it hits/misses L0") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETSPEC_L2;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:SPECLD_L2_Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.idx := in_msg.idx;
        out_msg.Prefetch := in_msg.Prefetch;
      } // enqueue
    } // peek
  }
  // Jiyong, MLDOM: SpecLoad_Mem hit/miss must send request to lower levels
  action(ij_issueGETSPEC_Mem_pass_L0, 'mp0', desc="Issue SpecLoad Mem to lower levels given it hits/misses L0") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETSPEC_Mem;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:SPECLD_Mem_Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.idx := in_msg.idx;
        out_msg.Prefetch := in_msg.Prefetch;
      } // enqueue
    } // peek
  }
  // Jiyong, MLDOM: SpecLoad_Perfect miss must send request to lower levels
  action(ij_issueGETSPEC_Perfect_pass_L0, 'pp0', desc="Issue SpecLoad Perfect to lower levels given it misses L0") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETSPEC_Perfect;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:SPECLD_Perfect_Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.idx := in_msg.idx;
        out_msg.Prefetch := in_msg.Prefetch;
      } // enqueue
    } // peek
  }
  // Jiyong, MLDOM: SpecLoad_PerfectUnsafe miss must send request to lower levels
  action(ij_issueGETSPEC_PerfectUnsafe_pass_L0, 'pup0', desc="Issue SpecLoad PerfectUnsafe to lower levels given it misses L0") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETSPEC_PerfectUnsafe;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:SPECLD_PerfectUnsafe_Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.idx := in_msg.idx;
        out_msg.Prefetch := in_msg.Prefetch;
      } // enqueue
    } // peek
  }

  action(b_issueGETX, "b", desc="Issue GETX") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETX;
        out_msg.Sender := machineID;
        DPRINTF(RubySlicc, "%s\n", machineID);
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);

        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.Prefetch := in_msg.Prefetch;
      }
    }
  }

  action(c_issueUPGRADE, "c", desc="Issue GETX") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:UPGRADE;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);

        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.Prefetch := in_msg.Prefetch;
      }
    }
  }

  action(pa_issuePfGETS, "pa", desc="Issue prefetch GETS") {
    peek(optionalQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETS;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.Prefetch := in_msg.Prefetch;
      }
    }
  }

  action(pb_issuePfGETX, "pb", desc="Issue prefetch GETX") {
    peek(optionalQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, CoherenceMsg, request_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:GETX;
        out_msg.Sender := machineID;
        DPRINTF(RubySlicc, "%s\n", machineID);
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);

        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Dest);
        out_msg.MessageSize := MessageSizeType:Control;
        out_msg.AccessMode := in_msg.AccessMode;
        out_msg.Prefetch := in_msg.Prefetch;
      }
    }
  }

  action(f_sendDataToL1, "f", desc="send data to the L2 cache") {
    enqueue(requestNetwork_out, CoherenceMsg, response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Class := CoherenceClass:INV_DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
      out_msg.Dest := createMachineID(MachineType:L1Cache, version);
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
    }
    cache_entry.Dirty := false;
  }

  action(fi_sendInvAck, "fi", desc="send data to the L2 cache") {
    peek(messgeBuffer_in, CoherenceMsg) {
      enqueue(requestNetwork_out, CoherenceMsg, response_latency) {
        out_msg.addr := address;
        out_msg.Class := CoherenceClass:INV_ACK;
        out_msg.Sender := machineID;
        out_msg.Dest := createMachineID(MachineType:L1Cache, version);
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(forward_eviction_to_cpu, "\cc", desc="sends eviction information to the processor") {
    if (send_evictions) {
      DPRINTF(RubySlicc, "Sending invalidation for %#x to the CPU\n", address);
      sequencer.evictionCallback(address, false);
    }
  }

  action(g_issuePUTX, "g", desc="send data to the L2 cache") {
    enqueue(requestNetwork_out, CoherenceMsg, response_latency) {
      assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Class := CoherenceClass:PUTX;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender:= machineID;
      out_msg.Dest := createMachineID(MachineType:L1Cache, version);

      if (cache_entry.Dirty) {
        out_msg.MessageSize := MessageSizeType:Writeback_Data;
        out_msg.DataBlk := cache_entry.DataBlk;
      } else {
        out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  // load hit in L0
  action(h_load_hit, "hd", desc="If not prefetch, notify sequencer the load completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
    Dcache.setMRU(cache_entry);
    DPRINTF(JY_Ruby, "load address: %#x hits in L0\n", address);
    sequencer.readCallback(address, cache_entry.DataBlk, false, true);     // Jiyong: load hit L0 in S/E/M state, hitAtL0
  }

  // ifetch hit in L0
  action(h_ifetch_hit, "hi", desc="If not prefetch, notify sequencer the ifetch completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
    Icache.setMRU(cache_entry);
    DPRINTF(JY_Ruby, "ifetch address: %#x hits in L0\n", address);
    sequencer.readCallback(address, cache_entry.DataBlk, false, true);     // Jiyong: ifetch hit L0 in S/E/M state, hitAtL0
  }

  // Jiyong, MLDOM: SpecLD hits L0
  action(hj_spec_load_hit_l0, "h0", desc="Spec load LX hits L0. Notify sequencer the load returns.") {
    peek(mandatoryQueue_in, RubyRequest) {
      assert(is_valid(cache_entry));
      DPRINTF(JY_Ruby, "SPEC_LD_LX (X>=0) hits L0: address: %#x, idx: %d; call readCallback_fromL0\n", address, in_msg.idx);
      sequencer.readCallbackObliv_fromL0(address, true, cache_entry.DataBlk, in_msg.idx);
    }
  }
  // Jiyong, MLDOM: SpecLD misses L0
  action(mj_spec_load_miss_l0, "m0", desc="Spec load LX misses L0. Notify sequencer the load returns.") {
    peek(mandatoryQueue_in, RubyRequest) {
      DPRINTF(JY_Ruby, "SPEC_LD_LX (X>=0) misss L0: address: %#x, idx: %d; call readCallback_fromL0\n", address, in_msg.idx);
      sequencer.readCallbackObliv_fromL0(address, false, cache_entry.DataBlk, in_msg.idx);
    }
  }

  // Jiyong, MLDOM: send the spec data from L1 up to the core
  action(hxj_send_spec_data_from_l1_to_cpu, "df1", desc="Send spec data from L1 up to the core. Notify sequencer the load returns.") {
    peek(messgeBuffer_specData_in, CoherenceMsg) {
      DPRINTF(JY_Ruby, "send the spec data from L1 upto the core: address: %#x, idx: %d ; call readCallback_fromL1\n", address, in_msg.idx);
      DataBlock dataBlk_L1  := in_msg.DataBlk_L1;
      sequencer.readCallbackObliv_fromL1(address, in_msg.hitAtL1, dataBlk_L1, in_msg.idx);
    }
  }

  // Jiyong, MLDOM: send the spec data from L2 up to the core
  action(hxj_send_spec_data_from_l2_to_cpu, "df2", desc="Send spec data from L2 up to the core. Notify sequencer the load returns.") {
    peek(messgeBuffer_specData_in, CoherenceMsg) {
      DPRINTF(JY_Ruby, "send the spec data from L2 upto the core: address: %#x, idx: %d ; call readCallback_fromL2\n", address, in_msg.idx);
      DataBlock dataBlk_L2  := in_msg.DataBlk_L2;
      sequencer.readCallbackObliv_fromL2(address, in_msg.hitAtL2, dataBlk_L2, in_msg.idx);
    }
  }

  // Jiyong, MLDOM: send the spec data from Mem up to the core
  action(hxj_send_spec_data_from_mem_to_cpu, "dfm", desc="Send spec data from Mem up to the core. Notify sequencer the load returns.") {
    peek(messgeBuffer_specData_in, CoherenceMsg) {
      DPRINTF(JY_Ruby, "send the spec data from Mem upto the core: address: %#x, idx: %d ; call readCallback_fromMem\n", address, in_msg.idx);
      DataBlock dataBlk_Mem  := in_msg.DataBlk_Mem;
      sequencer.readCallbackObliv_fromMem(address, in_msg.hitAtMem, dataBlk_Mem, in_msg.idx);
    }
  }

  action(hx_load_hit, "hxd", desc="notify sequencer the load completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
    Dcache.setMRU(cache_entry);
    sequencer.readCallback(address, cache_entry.DataBlk, true,
                        cache_entry.hitAtL0, cache_entry.hitAtL1, cache_entry.hitAtL2, cache_entry.hitAtMem);   // Jiyong: load miss L0, receive L1's hit status
    cache_entry.hitAtL0  := false;
    cache_entry.hitAtL1  := false;
    cache_entry.hitAtL2  := false;
    cache_entry.hitAtMem := false;
  }

  action(hx_ifetch_hit, "hxi", desc="notify sequencer the ifetch completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
    Icache.setMRU(cache_entry);
    sequencer.readCallback(address, cache_entry.DataBlk, true,
                        cache_entry.hitAtL0, cache_entry.hitAtL1, cache_entry.hitAtL2, cache_entry.hitAtMem);   // Jiyong: ifetch miss L0, receive L1's hit status
    cache_entry.hitAtL0  := false;
    cache_entry.hitAtL1  := false;
    cache_entry.hitAtL2  := false;
    cache_entry.hitAtMem := false;
  }

  action(hh_store_hit, "\h", desc="If not prefetch, notify sequencer that store completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
    Dcache.setMRU(cache_entry);
    DPRINTF(JY_Ruby, "address: %#x hits in L0\n", address);
    sequencer.writeCallback(address, cache_entry.DataBlk, false, true);    // Jiyong: store hit L0 in E/M state, hitAtL0
    cache_entry.Dirty := true;
  }

  action(hhx_store_hit, "\hx", desc="If not prefetch, notify sequencer that store completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
    Dcache.setMRU(cache_entry);
    sequencer.writeCallback(address, cache_entry.DataBlk, true,
                        cache_entry.hitAtL0, cache_entry.hitAtL1, cache_entry.hitAtL2, cache_entry.hitAtMem);   // Jiyong: store miss L0, receive L1's hit status
    cache_entry.Dirty := true;
    cache_entry.hitAtL0  := false;
    cache_entry.hitAtL1  := false;
    cache_entry.hitAtL2  := false;
    cache_entry.hitAtMem := false;
  }

  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(TBEs);
    assert(is_valid(cache_entry));
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
    tbe.isPrefetch := false;
  }

  action(k_popMandatoryQueue, "k", desc="Pop mandatory queue.") {
    mandatoryQueue_in.dequeue(clockEdge());
  }

  action(l_popRequestQueue, "l",
         desc="Pop incoming request queue and profile the delay within this virtual network") {
    Tick delay := messgeBuffer_in.dequeue(clockEdge());
    profileMsgDelay(2, ticksToCycles(delay));
  }

  action(o_popIncomingResponseQueue, "o",
         desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    Tick delay := messgeBuffer_in.dequeue(clockEdge());
    profileMsgDelay(1, ticksToCycles(delay));
  }

  // Jiyong, MLDOM: pop spec data response
  action(osd_popIncomingSpecDataResponseQueue, "osd",
         desc="Pop Incoming spec data Response queue and profile the delay within this virtual network") {
    Tick delay := messgeBuffer_specData_in.dequeue(clockEdge());
    profileMsgDelay(1, ticksToCycles(delay));
  }

  action(s_deallocateTBE, "s", desc="Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataToCache, "u", desc="Write data to cache") {
    peek(messgeBuffer_in, CoherenceMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk  := in_msg.DataBlk;
      cache_entry.hitAtL0  := in_msg.hitAtL0;
      cache_entry.hitAtL1  := in_msg.hitAtL1;
      cache_entry.hitAtL2  := in_msg.hitAtL2;
      cache_entry.hitAtMem := in_msg.hitAtMem;
    }
  }

  action(u_writeInstToCache, "ui", desc="Write data to cache") {
    peek(messgeBuffer_in, CoherenceMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk  := in_msg.DataBlk;
      cache_entry.hitAtL0  := in_msg.hitAtL0;
      cache_entry.hitAtL1  := in_msg.hitAtL1;
      cache_entry.hitAtL2  := in_msg.hitAtL2;
      cache_entry.hitAtMem := in_msg.hitAtMem;
    }
  }

  action(ff_deallocateCacheBlock, "\f",
         desc="Deallocate L1 cache block.") {
    if (Dcache.isTagPresent(address)) {
      Dcache.deallocate(address);
    } else {
      Icache.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateDCacheBlock, "\o", desc="Set L1 D-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Dcache.allocate(address, new Entry));
    }
  }

  action(pp_allocateICacheBlock, "\p", desc="Set L1 I-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(Icache.allocate(address, new Entry));
    }
  }

  action(z_stallAndWaitMandatoryQueue, "\z", desc="Stall and wait the L0 mandatory request queue") {
    stall_and_wait(mandatoryQueue_in, address);
  }

  action(z_stallAndWaitOptionalQueue, "\pz", desc="Stall and wait the L0 prefetch request queue") {
    stall_and_wait(optionalQueue_in, address);
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpAllBuffers(address);
  }

  action(uu_profileInstMiss, "\ui", desc="Profile the demand miss") {
        ++Icache.demand_misses;
  }

  action(uu_profileInstHit, "\uih", desc="Profile the demand miss") {
        ++Icache.demand_hits;
  }

  action(uu_profileDataMiss, "\ud", desc="Profile the demand miss") {
        ++Dcache.demand_misses;
  }

  action(uu_profileDataHit, "\udh", desc="Profile the demand miss") {
        ++Dcache.demand_hits;
  }

  action(po_observeHit, "\ph", desc="Inform the prefetcher about the hit") {
    peek(mandatoryQueue_in, RubyRequest) {
      if (cache_entry.isPrefetch) {
        DPRINTF(JY_Ruby, "observeHit: address: %#x, idx: %d\n", in_msg.LineAddress, in_msg.idx);
        prefetcher.observePfHit(in_msg.LineAddress);
        cache_entry.isPrefetch := false;
      }
    }
  }

  action(po_observeMiss, "\po", desc="Inform the prefetcher about the miss") {
    peek(mandatoryQueue_in, RubyRequest) {
      if (enable_prefetch) {
        DPRINTF(JY_Ruby, "observeMiss: address: %#x, idx: %d\n", in_msg.LineAddress, in_msg.idx);
        prefetcher.observeMiss(in_msg.LineAddress, in_msg.Type);
      }
    }
  }

  action(ppm_observePfMiss, "\ppm", desc="Inform the prefetcher about the partial miss") {
    peek(mandatoryQueue_in, RubyRequest) {
      DPRINTF(JY_Ruby, "observePfMiss: address: %#x, idx: %d\n", in_msg.LineAddress, in_msg.idx);
      prefetcher.observePfMiss(in_msg.LineAddress);
    }
  }

  action(pq_popPrefetchQueue, "\pq", desc="Pop the prefetch request queue") {
    optionalQueue_in.dequeue(clockEdge());
  }

  action(mp_markPrefetched, "mp", desc="Write data from response queue to cache") {
    assert(is_valid(cache_entry));
    cache_entry.isPrefetch := true;
  }

  //*****************************************************
  // TRANSITIONS
  //*****************************************************

  /*** Jiyong, MLDOM: transitions for SpecLoad_LX Begin ***/
  // SpecLD_L0 Hit in L0, Only need to return hit
  transition({S,E,M}, SpecLoad_L0) {
    // return the data immediately
    hj_spec_load_hit_l0;
    k_popMandatoryQueue;
  }

  // SpecLD_L0 Miss in L0, Only need to return miss (not returning the stale data for now)
  transition({I,IS,IM,SM,Inst_IS,PF_IS,PF_Inst_IS,PF_IM}, SpecLoad_L0) {
    // return the miss immediately
    mj_spec_load_miss_l0;
    k_popMandatoryQueue;
  }

  // SpecLD_L1 Hit in L0, return hit and issue request to L1
  transition({S,E,M}, SpecLoad_L1) {
    // return the data immediately
    hj_spec_load_hit_l0;
    // send a request to L1
    ij_issueGETSPEC_L1_pass_L0;
    k_popMandatoryQueue;
  }

  // SpecLD_L1 Miss in L0, return miss and issue request to L1
  transition({I,IS,IM,SM,Inst_IS,PF_IS,PF_Inst_IS,PF_IM}, SpecLoad_L1) {
    // return the miss immediately
    mj_spec_load_miss_l0;
    // send a request to L1
    ij_issueGETSPEC_L1_pass_L0;
    k_popMandatoryQueue;
  }

  // SpecLD_2 Hit in L0, return hit and issue request to L1(L2)
  transition({S,E,M}, SpecLoad_L2) {
    // return the data immediately
    hj_spec_load_hit_l0;
    // send a request to L1(L2)
    ij_issueGETSPEC_L2_pass_L0;
    k_popMandatoryQueue;
  }

  // SpecLD_2 Miss in L0, return miss and issue request to L1(L2)
  transition({I,IS,IM,SM,Inst_IS,PF_IS,PF_Inst_IS,PF_IM}, SpecLoad_L2) {
    // return the miss immediately
    mj_spec_load_miss_l0;
    // send a request to L1(L2)
    ij_issueGETSPEC_L2_pass_L0;
    k_popMandatoryQueue;
  }

  // SpecLD_Mem Hit in L0, return hit and issue request to L1(L2+Mem)
  transition({S,E,M}, SpecLoad_Mem) {
    // return the data immediately
    hj_spec_load_hit_l0;
    // send a request to L1(L2+Mem)
    ij_issueGETSPEC_Mem_pass_L0;
    k_popMandatoryQueue;
  }

  // SpecLD_Mem Miss in L0, return miss and issue request to L1(L2+Mem)
  transition({I,IS,IM,SM,Inst_IS,PF_IS,PF_Inst_IS,PF_IM}, SpecLoad_Mem) {
    // return the miss immediately
    mj_spec_load_miss_l0;
    // send a request to L1(L2)
    ij_issueGETSPEC_Mem_pass_L0;
    k_popMandatoryQueue;
  }

  // SpecLD_Perfect Hit in L0, return hit and done
  transition({S,E,M}, SpecLoad_Perfect) {
    // return data immediately
    hj_spec_load_hit_l0;
    k_popMandatoryQueue;
  }

  // SpecLD_Perfect Miss in L0, return miss and issue request to L1
  transition({I,IS,IM,SM,Inst_IS,PF_IS,PF_Inst_IS,PF_IM}, SpecLoad_Perfect) {
    // return the miss immediately
    mj_spec_load_miss_l0;
    // send a request to L1
    ij_issueGETSPEC_Perfect_pass_L0;
    k_popMandatoryQueue;
  }

  // SpecLD_PerfectUnsafe Hit in L0, return hit and done
  transition({S,E,M}, SpecLoad_PerfectUnsafe) {
    // return data immediately
    hj_spec_load_hit_l0;
    k_popMandatoryQueue;
  }

  // SpecLD_PerfectUnsafe wait on transient state
  transition({Inst_IS,IS,IM,SM,PF_IS,PF_Inst_IS,PF_IM}, SpecLoad_PerfectUnsafe) {
    z_stallAndWaitMandatoryQueue;
  }

  // SpecLD_Perfect Miss in L0, return miss and issue request to L1
  transition({I}, SpecLoad_PerfectUnsafe) {
    // return the miss immediately
    mj_spec_load_miss_l0;
    // send a request to L1
    ij_issueGETSPEC_PerfectUnsafe_pass_L0;
    k_popMandatoryQueue;
  }
  /*** Jiyong, MLDOM: transitions for SpecLoad_LX End ***/

  // Transitions for Load/Store/Replacement/WriteBack from transient states
  transition({Inst_IS, IS, IM, SM}, {Load, Ifetch, Store, L0I_Replacement, L0D_Replacement}) {
    z_stallAndWaitMandatoryQueue;
  }

  // Transitions from Idle
  transition(I, Load, IS) { //L0DTagArrayRead} {
    oo_allocateDCacheBlock;
    i_allocateTBE;
    a_issueGETS;
    uu_profileDataMiss;
    po_observeMiss;
    k_popMandatoryQueue;
  }

  transition(I, Ifetch, Inst_IS) { //L0ITagArrayRead} {
    pp_allocateICacheBlock;
    i_allocateTBE;
    a_issueGETS;
    uu_profileInstMiss;
    po_observeMiss;
    k_popMandatoryQueue;
  }

  transition(I, Store, IM) {  //L0DTagArrayRead} {
    oo_allocateDCacheBlock;
    i_allocateTBE;
    b_issueGETX;
    uu_profileDataMiss;
    po_observeMiss;
    k_popMandatoryQueue;
  }

  transition({I, IS, IM, Inst_IS}, Inv) {} {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(SM, Inv, IM) { //L0D} {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  // Transitions from Shared
  transition({S,E,M}, Load) { //L0DTagArrayRead, L0DDataArrayRead} {
    h_load_hit;
    uu_profileDataHit;
    po_observeHit;
    k_popMandatoryQueue;
  }

  transition({S,E,M}, Ifetch) { //L0DTagArrayRead, L0DDataArrayRead} {
    h_ifetch_hit;
    uu_profileInstHit;
    po_observeHit;
    k_popMandatoryQueue;
  }

  transition(S, Store, SM) { //L0DTagArrayRead, L0DDataArrayWrite}{
    i_allocateTBE;
    c_issueUPGRADE;
    uu_profileDataMiss;
    k_popMandatoryQueue;
  }

  transition(S, {L0D_Replacement, PF_L0D_Replacement}, I) { // {L0DTagArrayRead} {
    forward_eviction_to_cpu;
    ff_deallocateCacheBlock;
  }

  transition(S, {L0I_Replacement, PF_L0I_Replacement}, I) { // {L0ITagArrayRead} {
    forward_eviction_to_cpu;
    ff_deallocateCacheBlock;
  }

  transition(S, Inv, I) {} {
    forward_eviction_to_cpu;
    fi_sendInvAck;
    ff_deallocateCacheBlock;
    l_popRequestQueue;
  }

  // Transitions from Exclusive
  transition({E,M}, Store, M) {} {
    hh_store_hit;
    uu_profileDataHit;
    po_observeHit;
    k_popMandatoryQueue;
  }

  transition(E, {L0D_Replacement, PF_L0D_Replacement}, I) { // {L0DTagArrayRead} {
    forward_eviction_to_cpu;
    g_issuePUTX;
    ff_deallocateCacheBlock;
  }

  transition(E, {L0I_Replacement, PF_L0I_Replacement}, I) { // {L0ITagArrayRead} {
    forward_eviction_to_cpu;
    g_issuePUTX;
    ff_deallocateCacheBlock;
  }

  transition(E, {Inv, Fwd_GETX}, I) {} {
    // don't send data
    forward_eviction_to_cpu;
    fi_sendInvAck;
    ff_deallocateCacheBlock;
    l_popRequestQueue;
  }

  transition(E, {Fwd_GETS, Fwd_GET_INSTR}, S) {} {
    f_sendDataToL1;
    l_popRequestQueue;
  }

  // Transitions from Modified
  transition(M, {L0D_Replacement, PF_L0D_Replacement}, I) { // {L0DTagArrayRead} {
    forward_eviction_to_cpu;
    g_issuePUTX;
    ff_deallocateCacheBlock;
  }

  transition(M, {L0I_Replacement, PF_L0I_Replacement}, I) { // {L0ITagArrayRead} {
    forward_eviction_to_cpu;
    g_issuePUTX;
    ff_deallocateCacheBlock;
  }

  transition(M, {Inv, Fwd_GETX}, I) {} {
    forward_eviction_to_cpu;
    f_sendDataToL1;
    ff_deallocateCacheBlock;
    l_popRequestQueue;
  }

  transition(M, {Fwd_GETS, Fwd_GET_INSTR}, S) {} {
    f_sendDataToL1;
    l_popRequestQueue;
  }

  /// transient states

  transition(IS, Data, S) { // {L0DTagArrayWrite, L0DDataArrayWrite} {
    u_writeDataToCache;
    hx_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, Data_Exclusive, E) { // {L0DTagArrayWrite} {
    u_writeDataToCache;
    hx_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(Inst_IS, Data, S) {} {
    u_writeInstToCache;
    hx_ifetch_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(Inst_IS, Data_Exclusive, E) {} {
    u_writeInstToCache;
    hx_ifetch_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition({IM,SM}, Data_Exclusive, M) {} {
    u_writeDataToCache;
    hhx_store_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // Jiyong, MLDOM: forward the spec data up
  transition({I,S,E,M,Inst_IS,IS,IM,SM,PF_IS,PF_Inst_IS,PF_IM}, Data_Spec_fromL1) {
    hxj_send_spec_data_from_l1_to_cpu;
    osd_popIncomingSpecDataResponseQueue;
  }

  // Jiyong, MLDOM: forward the spec data up
  transition({I,S,E,M,Inst_IS,IS,IM,SM,PF_IS,PF_Inst_IS,PF_IM}, Data_Spec_fromL2) {
    hxj_send_spec_data_from_l2_to_cpu;
    osd_popIncomingSpecDataResponseQueue;
  }

  // Jiyong, MLDOM: forward the spec data up
  transition({I,S,E,M,Inst_IS,IS,IM,SM,PF_IS,PF_Inst_IS,PF_IM}, Data_Spec_fromMem) {
    hxj_send_spec_data_from_mem_to_cpu;
    osd_popIncomingSpecDataResponseQueue;
  }

  //*****************************************************
  //  Prefetch transitions
  //****************************************************
  transition({PF_Inst_IS, PF_IS}, {Store, L0I_Replacement, L0D_Replacement}) {
    z_stallAndWaitMandatoryQueue;
  }

  transition(PF_IM, {Load, Ifetch, L0I_Replacement, L0D_Replacement}) {
    z_stallAndWaitMandatoryQueue;
  }

  transition({Inst_IS,IS,IM,SM,PF_IS,PF_Inst_IS,PF_IM}, {PF_L0D_Replacement, PF_L0I_Replacement}) {
    z_stallAndWaitOptionalQueue;
  }

  // Transitions from Idle
  transition(I, {L0I_Replacement, L0D_Replacement, PF_L0I_Replacement, PF_L0D_Replacement}) {
    ff_deallocateCacheBlock;
  }

  transition({S,E,M,IS,Inst_IS,IM,SM,PF_Inst_IS,PF_IS,PF_IM},
             {PF_Load, PF_Store, PF_Ifetch}) {
    pq_popPrefetchQueue;
  }

  transition(I, PF_Load, PF_IS) {
    oo_allocateDCacheBlock;
    i_allocateTBE;
    pa_issuePfGETS;
    pq_popPrefetchQueue;
  }

  transition(I, PF_Ifetch, PF_Inst_IS) {
    pp_allocateICacheBlock;
    i_allocateTBE;
    pa_issuePfGETS;
    pq_popPrefetchQueue;
  }

  transition(I, PF_Store, PF_IM) {
    oo_allocateDCacheBlock;
    i_allocateTBE;
    pb_issuePfGETX;
    pq_popPrefetchQueue;
  }

  transition({PF_IS, PF_Inst_IS, PF_IM}, Inv) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(PF_IS, Load, IS) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition(PF_Inst_IS, Load, IS) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition(PF_IS, Ifetch, Inst_IS) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition(PF_Inst_IS, Ifetch, Inst_IS) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition(PF_IM, Store, IM) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition(PF_IS, Data, S) {
    u_writeDataToCache;
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS, Data_Exclusive, E) {
    u_writeDataToCache;
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_Inst_IS, Data, S) {
    u_writeDataToCache;
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_Inst_IS, Data_Exclusive, E) {
    u_writeDataToCache;
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IM, Data_Exclusive, M) {
    u_writeDataToCache;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }
}

